{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Tensorfow_SpellChecker.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dipansh-girdhar/NLP/blob/master/Tensorfow_SpellChecker.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "343kCNsrM2p-",
        "colab_type": "text"
      },
      "source": [
        "## Spelling Correction using Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0y5-4zgM2qC",
        "colab_type": "text"
      },
      "source": [
        "The sections of the project are:\n",
        "- Loading the Data\n",
        "- Preparing the Data\n",
        "- Building the Model\n",
        "- Training the Model\n",
        "- Fixing Custom Sentences\n",
        "- Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq05_hYSQnXj",
        "colab_type": "code",
        "outputId": "9689ae23-da62-4e3c-a1f0-454b7417b460",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360
        }
      },
      "source": [
        "pip install tensorflow==1.1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/e4/b2a8bcd1fa689489050386ec70c5c547e4a75d06f2cc2b55f45463cd092c/tensorflow-1.1.0-cp36-cp36m-manylinux1_x86_64.whl (31.4MB)\n",
            "\u001b[K     |████████████████████████████████| 31.4MB 158kB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (1.18.3)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.1) (0.34.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.1) (46.1.3)\n",
            "Installing collected packages: tensorflow\n",
            "  Found existing installation: tensorflow 1.2.0\n",
            "    Uninstalling tensorflow-1.2.0:\n",
            "      Successfully uninstalled tensorflow-1.2.0\n",
            "Successfully installed tensorflow-1.1.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvgU5oTMM2qD",
        "colab_type": "code",
        "outputId": "608a1f11-888c-4177-8b44-233af024e5b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# importing the packages\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "from collections import namedtuple\n",
        "from tensorflow.python.layers.core import Dense\n",
        "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
        "import time\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vTGt0sKM2qR",
        "colab_type": "text"
      },
      "source": [
        "### Loading the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4ZFMrZiM2qT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to Load the book from his files\n",
        "def load_book(path):    \n",
        "    input_file = os.path.join(path)\n",
        "    with open(input_file) as f:\n",
        "        book = f.read()\n",
        "    return book"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7HSBTQ9M2qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Collect all of the book file names\n",
        "path = 'books/'\n",
        "book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
        "# book_files = book_files[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiZxuy5bNrjH",
        "colab_type": "code",
        "outputId": "4ee40e43-83d5-4614-f0d5-bb90dbd18e52",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "book_files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['medical_sentences.txt']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc2INgKqM2qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the books using the file names\n",
        "books = []\n",
        "for book in book_files:\n",
        "    books.append(load_book(path+book))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ2w0P-lM2qu",
        "colab_type": "code",
        "outputId": "b46846e6-bd4e-4a67-a64a-b250985aa5e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Getting number of words in each book \n",
        "for i in range(len(books)):\n",
        "    print(\"The number of {} words in book of named {}.\".format(len(books[i].split()), book_files[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The number of 787536 words in book of named medical_sentences.txt.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-lO2h7kM2q2",
        "colab_type": "code",
        "outputId": "6590cd9a-b54e-41ce-a255-0b134b4bdff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# print 500 words of 1st books \n",
        "books[0][:500]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Patient with wife who admits they eat ice cream every night patient reports that hes had a hx of anemia and wbc issues, states that he has just been diagnosed with CMML and is going to start chemo under care of.\\nSKIN: senile purpura.\\n1. Chronic myelomonocytic leukemia not having achieved remission - C93.10.\\n2. HTN w CHF CKD 1-4 (Benign Hypertensive heart and chronic kidney disease with heart failure and stage 1-4 chronic kidney disease) - I13.0, 2nd to.\\n**Possible Combo Codes: I509, I130, E1122,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5NTdzbJM2rC",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhBa442cM2rD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to clean the data\n",
        "def clean_text(text):    \n",
        "    text = re.sub(r'\\n', ' ', text) \n",
        "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
        "    text = re.sub('a0','', text)\n",
        "    text = re.sub('\\'92t','\\'t', text)\n",
        "    text = re.sub('\\'92s','\\'s', text)\n",
        "    text = re.sub('\\'92m','\\'m', text)\n",
        "    text = re.sub('\\'92ll','\\'ll', text)\n",
        "    text = re.sub('\\'91','', text)\n",
        "    text = re.sub('\\'92','', text)\n",
        "    text = re.sub('\\'93','', text)\n",
        "    text = re.sub('\\'94','', text)\n",
        "    text = re.sub('\\.','. ', text)\n",
        "    text = re.sub('\\!','! ', text)\n",
        "    text = re.sub('\\?','? ', text)\n",
        "    text = re.sub(' +',' ', text)\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yh8aON3fM2rJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Clean the text of the books\n",
        "clean_books = []\n",
        "for book in books:\n",
        "    clean_books.append(clean_text(book))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "piac3mIDM2rV",
        "colab_type": "code",
        "outputId": "0a65f947-c88f-4a86-9015-817b2d85f108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Check to ensure the text has been cleaned properly\n",
        "clean_books[0][:500]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Patient with wife who admits they eat ice cream every night patient reports that hes had a hx of anemia and wbc issues, states that he has just been diagnosed with CMML and is going to start chemo under care of. SKIN: senile purpura. 1. Chronic myelomonocytic leukemia not having achieved remission - C93. 10. 2. HTN w CHF CKD 1-4 Benign Hypertensive heart and chronic kidney disease with heart failure and stage 1-4 chronic kidney disease - I13. 0, 2nd to. Possible Combo Codes: I509, I130, E1122, N'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC5WeGrBM2rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a dictionary to convert the vocabulary (characters) to integers\n",
        "vocab_to_int = {}\n",
        "count = 0\n",
        "for book in clean_books:\n",
        "    for character in book:\n",
        "        if character not in vocab_to_int:\n",
        "            vocab_to_int[character] = count\n",
        "            count += 1\n",
        "\n",
        "# Add special tokens to vocab_to_int\n",
        "codes = ['<PAD>','<EOS>','<GO>']\n",
        "for code in codes:\n",
        "    vocab_to_int[code] = count\n",
        "    count += 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fZW30dBM2rn",
        "colab_type": "code",
        "outputId": "d8afd5b4-ab63-472d-eb25-a87f551faa5e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Check the size of vocabulary and all of the values\n",
        "vocab_size = len(vocab_to_int)\n",
        "print(\"The vocabulary contains {} characters.\".format(vocab_size))\n",
        "print(sorted(vocab_to_int))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The vocabulary contains 104 characters.\n",
            "[' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '<EOS>', '<GO>', '<PAD>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '^', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '~', '£', '§', '«', '¬', '®', '°', '»', '—', '‘', '’', '“', '”', '„', '•', '€', '™', '■', '□', '►', '♦', '✓', '\\uf06c']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7yqfwZ3M2ru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create another dictionary to convert integers to their respective characters\n",
        "int_to_vocab = {}\n",
        "for character, value in vocab_to_int.items():\n",
        "    int_to_vocab[value] = character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umO6Cbc8M2r0",
        "colab_type": "code",
        "outputId": "1397e528-1c9b-4b46-f0a2-6a63f87914bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Split the text from the books into sentences \n",
        "sentences = []\n",
        "for book in clean_books:\n",
        "    for sentence in book.split('. '):\n",
        "        sentences.append(sentence + '.')\n",
        "print(\"Total number of sentences are {} .\".format(len(sentences)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of sentences are 112350 .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3LZ5-lcM2r6",
        "colab_type": "code",
        "outputId": "111e67a0-4369-4b53-baa0-4d87d11f1496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# check for sentence splitting correctly \n",
        "sentences[:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Patient with wife who admits they eat ice cream every night patient reports that hes had a hx of anemia and wbc issues, states that he has just been diagnosed with CMML and is going to start chemo under care of.',\n",
              " 'SKIN: senile purpura.',\n",
              " '1.',\n",
              " 'Chronic myelomonocytic leukemia not having achieved remission - C93.',\n",
              " '10.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKr_4UACM2sC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert sentences to integers\n",
        "int_sentences = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    int_sentence = []\n",
        "    for character in sentence:\n",
        "        int_sentence.append(vocab_to_int[character])\n",
        "    int_sentences.append(int_sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BC-mGiHM2sI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Find the length of each sentence\n",
        "lengths = []\n",
        "for sentence in int_sentences:\n",
        "    lengths.append(len(sentence))\n",
        "lengths = pd.DataFrame(lengths, columns=[\"counts\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OMUxEc_M2sV",
        "colab_type": "code",
        "outputId": "8bf59bce-a76c-4dbf-bfc7-5e4c926a4e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "lengths.describe()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>counts</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>112350.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>44.299991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>85.666529</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>29.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>55.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1687.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              counts\n",
              "count  112350.000000\n",
              "mean       44.299991\n",
              "std        85.666529\n",
              "min         1.000000\n",
              "25%        12.000000\n",
              "50%        29.000000\n",
              "75%        55.000000\n",
              "max      1687.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajBkR-rrM2sc",
        "colab_type": "code",
        "outputId": "3e3adf45-9e0b-44ab-ac52-a0dab978a580",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Limit the data we will use to train our model\n",
        "max_length = 92\n",
        "min_length = 10\n",
        "\n",
        "good_sentences = []\n",
        "\n",
        "for sentence in int_sentences:\n",
        "    if len(sentence) <= max_length and len(sentence) >= min_length:\n",
        "        good_sentences.append(sentence)\n",
        "\n",
        "print(\"Total number of {} sentences to train and test our model.\".format(len(good_sentences)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of 77079 sentences to train and test our model.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETBC_bypM2sl",
        "colab_type": "code",
        "outputId": "145db1cd-5d48-4680-e576-52c7b4aaa57b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Split the data into training and testing sentences\n",
        "training, testing = train_test_split(good_sentences, test_size = 0.15, random_state = 2)\n",
        "\n",
        "print(\"Number of training sentences:\", len(training))\n",
        "print(\"Number of testing sentences:\", len(testing))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sentences: 65517\n",
            "Number of testing sentences: 11562\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icNrw8mlM2ss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sort the sentences by length to reduce padding, which will allow the model to train faster\n",
        "training_sorted = []\n",
        "testing_sorted = []\n",
        "\n",
        "for i in range(min_length, max_length+1):\n",
        "    for sentence in training:\n",
        "        if len(sentence) == i:\n",
        "            training_sorted.append(sentence)\n",
        "    for sentence in testing:\n",
        "        if len(sentence) == i:\n",
        "            testing_sorted.append(sentence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8Zr0VaHM2s4",
        "colab_type": "code",
        "outputId": "aa4315ca-11c6-4daa-a4ac-ff4a9900acff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Check to ensure the sentences have been selected and sorted correctly\n",
        "for i in range(5):\n",
        "    print(training_sorted[i], len(training_sorted[i]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[38, 6, 0, 16, 3, 12, 1, 16, 14, 28] 10\n",
            "[50, 6, 0, 16, 3, 12, 1, 16, 14, 28] 10\n",
            "[39, 23, 6, 41, 5, 11, 6, 2, 10, 28] 10\n",
            "[50, 6, 0, 16, 3, 12, 1, 16, 14, 28] 10\n",
            "[40, 6, 0, 16, 3, 12, 1, 16, 14, 28] 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvNVLDHGM2s-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to Relocate, remove, or add characters to create spelling mistakes\n",
        "letters = ['a','b','c','d','e','f','g','h','i','j','k','l','m',\n",
        "           'n','o','p','q','r','s','t','u','v','w','x','y','z',]\n",
        "\n",
        "def noise_maker(sentence, threshold):   \n",
        "    \n",
        "    noisy_sentence = []\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        random = np.random.uniform(0,1,1)\n",
        "        # Most characters will be correct since the threshold value is high\n",
        "        if random < threshold:\n",
        "            noisy_sentence.append(sentence[i])\n",
        "        else:\n",
        "            new_random = np.random.uniform(0,1,1)\n",
        "            # ~33% chance characters will swap locations\n",
        "            if new_random > 0.67:\n",
        "                if i == (len(sentence) - 1):\n",
        "                    # If last character in sentence, it will not be typed\n",
        "                    continue\n",
        "                else:\n",
        "                    # if any other character, swap order with following character\n",
        "                    noisy_sentence.append(sentence[i+1])\n",
        "                    noisy_sentence.append(sentence[i])\n",
        "                    i += 1\n",
        "            # ~33% chance an extra lower case letter will be added to the sentence\n",
        "            elif new_random < 0.33:\n",
        "                random_letter = np.random.choice(letters, 1)[0]\n",
        "                noisy_sentence.append(vocab_to_int[random_letter])\n",
        "                noisy_sentence.append(sentence[i])\n",
        "            # ~33% chance a character will not be typed\n",
        "            else:\n",
        "                pass     \n",
        "        i += 1\n",
        "    return noisy_sentence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LDvJ32vtM2tI",
        "colab_type": "code",
        "outputId": "003daff9-e1bd-4bfa-9af7-932db3364ff4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# Check to ensure noise_maker is making mistakes correctly.\n",
        "threshold = 0.9\n",
        "for sentence in training_sorted[:5]:\n",
        "    print(sentence)\n",
        "    print(noise_maker(sentence, threshold))\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[38, 6, 0, 16, 3, 12, 1, 16, 14, 28]\n",
            "[38, 17, 6, 0, 3, 12, 1, 16, 14, 28]\n",
            "\n",
            "[50, 6, 0, 16, 3, 12, 1, 16, 14, 28]\n",
            "[50, 6, 0, 16, 3, 12, 1, 16, 14, 28]\n",
            "\n",
            "[39, 23, 6, 41, 5, 11, 6, 2, 10, 28]\n",
            "[39, 23, 6, 41, 5, 11, 6, 2, 10, 28]\n",
            "\n",
            "[50, 6, 0, 16, 3, 12, 1, 16, 14, 28]\n",
            "[50, 6, 0, 16, 12, 1, 16, 14, 28]\n",
            "\n",
            "[40, 6, 0, 16, 3, 12, 1, 16, 14, 28]\n",
            "[40, 6, 22, 0, 16, 3, 12, 1, 16, 28]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "voSe8h_FM2tQ",
        "colab_type": "text"
      },
      "source": [
        "### Building the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98YW0CpOM2tS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to create placeholder\n",
        "def model_inputs():  \n",
        "    \n",
        "    with tf.name_scope('inputs'):\n",
        "        inputs = tf.placeholder(tf.int32, [None, None], name='inputs')\n",
        "    with tf.name_scope('targets'):\n",
        "        targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
        "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "    inputs_length = tf.placeholder(tf.int32, (None,), name='inputs_length')\n",
        "    targets_length = tf.placeholder(tf.int32, (None,), name='targets_length')\n",
        "    max_target_length = tf.reduce_max(targets_length, name='max_target_len')\n",
        "\n",
        "    return inputs, targets, keep_prob, inputs_length, targets_length, max_target_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXqkZTFyM2ta",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove the last word id from each batch and concat the <GO> to the begining of each batch\n",
        "def process_encoding_input(targets, vocab_to_int, batch_size):   \n",
        "    \n",
        "    with tf.name_scope(\"process_encoding\"):\n",
        "        ending = tf.strided_slice(targets, [0, 0], [batch_size, -1], [1, 1])\n",
        "        dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
        "\n",
        "    return dec_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KakDnjxQM2tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the encoding layer\n",
        "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob, direction):   \n",
        "    \n",
        "    if direction == 1:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_1D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "                    lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "\n",
        "                    drop = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.dynamic_rnn(drop, \n",
        "                                                              rnn_inputs,\n",
        "                                                              sequence_length,\n",
        "                                                              dtype=tf.float32)\n",
        "\n",
        "            return enc_output, enc_state\n",
        "        \n",
        "        \n",
        "    if direction == 2:\n",
        "        with tf.name_scope(\"RNN_Encoder_Cell_2D\"):\n",
        "            for layer in range(num_layers):\n",
        "                with tf.variable_scope('encoder_{}'.format(layer)):\n",
        "                    cell_fw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    cell_bw = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                    cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
        "                                                            input_keep_prob = keep_prob)\n",
        "\n",
        "                    enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
        "                                                                            cell_bw, \n",
        "                                                                            rnn_inputs,\n",
        "                                                                            sequence_length,\n",
        "                                                                            dtype=tf.float32)\n",
        "            # Join outputs since we are using a bidirectional RNN\n",
        "            enc_output = tf.concat(enc_output,2)\n",
        "            # Use only the forward state because the model can't use both states at once\n",
        "            return enc_output, enc_state[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4IhwtKPeM2tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the training logits\n",
        "def training_decoding_layer(dec_embed_input, targets_length, dec_cell, initial_state, output_layer, \n",
        "                            vocab_size, max_target_length):    \n",
        "    \n",
        "    with tf.name_scope(\"Training_Decoder\"):\n",
        "        training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
        "                                                            sequence_length=targets_length,\n",
        "                                                            time_major=False)\n",
        "\n",
        "        training_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                           training_helper,\n",
        "                                                           initial_state,\n",
        "                                                           output_layer) \n",
        "\n",
        "        training_logits, _ = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
        "                                                               output_time_major=False,\n",
        "                                                               impute_finished=True,\n",
        "                                                               maximum_iterations=max_target_length)\n",
        "        return training_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvEUqvOnM2tu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the inference logits\n",
        "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, initial_state, output_layer,\n",
        "                             max_target_length, batch_size):    \n",
        "    \n",
        "    with tf.name_scope(\"Inference_Decoder\"):\n",
        "        start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
        "\n",
        "        inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
        "                                                                    start_tokens,\n",
        "                                                                    end_token)\n",
        "\n",
        "        inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
        "                                                            inference_helper,\n",
        "                                                            initial_state,\n",
        "                                                            output_layer)\n",
        "\n",
        "        inference_logits, _ = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
        "                                                                output_time_major=False,\n",
        "                                                                impute_finished=True,\n",
        "                                                                maximum_iterations=max_target_length)\n",
        "\n",
        "        return inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jdwplEUM2t2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create the decoding cell and attention for the training and inference decoding layers\n",
        "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, inputs_length, targets_length, \n",
        "                   max_target_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers, direction):  \n",
        "    \n",
        "    with tf.name_scope(\"RNN_Decoder_Cell\"):\n",
        "        for layer in range(num_layers):\n",
        "            with tf.variable_scope('decoder_{}'.format(layer)):\n",
        "                lstm = tf.contrib.rnn.LSTMCell(rnn_size)\n",
        "                dec_cell = tf.contrib.rnn.DropoutWrapper(lstm, \n",
        "                                                         input_keep_prob = keep_prob)\n",
        "    \n",
        "    output_layer = Dense(vocab_size,\n",
        "                         kernel_initializer = tf.truncated_normal_initializer(mean = 0.0, stddev=0.1))\n",
        "    \n",
        "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
        "                                                  enc_output,\n",
        "                                                  inputs_length,\n",
        "                                                  normalize=False,\n",
        "                                                  name='BahdanauAttention')\n",
        "    \n",
        "    with tf.name_scope(\"Attention_Wrapper\"):\n",
        "        dec_cell = tf.contrib.seq2seq.DynamicAttentionWrapper(dec_cell,\n",
        "                                                              attn_mech,\n",
        "                                                              rnn_size)\n",
        "    \n",
        "    initial_state = tf.contrib.seq2seq.DynamicAttentionWrapperState(enc_state,\n",
        "                                                                    _zero_state_tensors(rnn_size, \n",
        "                                                                                        batch_size, \n",
        "                                                                                        tf.float32))\n",
        "\n",
        "    with tf.variable_scope(\"decode\"):\n",
        "        training_logits = training_decoding_layer(dec_embed_input, \n",
        "                                                  targets_length, \n",
        "                                                  dec_cell, \n",
        "                                                  initial_state,\n",
        "                                                  output_layer,\n",
        "                                                  vocab_size, \n",
        "                                                  max_target_length)\n",
        "    with tf.variable_scope(\"decode\", reuse=True):\n",
        "        inference_logits = inference_decoding_layer(embeddings,  \n",
        "                                                    vocab_to_int['<GO>'], \n",
        "                                                    vocab_to_int['<EOS>'],\n",
        "                                                    dec_cell, \n",
        "                                                    initial_state, \n",
        "                                                    output_layer,\n",
        "                                                    max_target_length,\n",
        "                                                    batch_size)\n",
        "\n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcAN6T9EM2t7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the previous functions to create the training and inference logits\n",
        "def seq2seq_model(inputs, targets, keep_prob, inputs_length, targets_length, max_target_length, \n",
        "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size, embedding_size, direction):    \n",
        "    \n",
        "    enc_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    enc_embed_input = tf.nn.embedding_lookup(enc_embeddings, inputs)\n",
        "    enc_output, enc_state = encoding_layer(rnn_size, inputs_length, num_layers, \n",
        "                                           enc_embed_input, keep_prob, direction)\n",
        "    \n",
        "    dec_embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1, 1))\n",
        "    dec_input = process_encoding_input(targets, vocab_to_int, batch_size)\n",
        "    dec_embed_input = tf.nn.embedding_lookup(dec_embeddings, dec_input)\n",
        "    \n",
        "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
        "                                                        dec_embeddings,\n",
        "                                                        enc_output,\n",
        "                                                        enc_state, \n",
        "                                                        vocab_size, \n",
        "                                                        inputs_length, \n",
        "                                                        targets_length, \n",
        "                                                        max_target_length,\n",
        "                                                        rnn_size, \n",
        "                                                        vocab_to_int, \n",
        "                                                        keep_prob, \n",
        "                                                        batch_size,\n",
        "                                                        num_layers,\n",
        "                                                        direction)\n",
        "    \n",
        "    return training_logits, inference_logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MA82jHrUM2uB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pad sentences with <PAD> so that each sentence of a batch has the same length\n",
        "def pad_sentence_batch(sentence_batch):    \n",
        "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
        "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9893-8lgM2uF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch sentences, noisy sentences, and the lengths of their sentences together.With each epoch, sentences will receive new mistakes\"\n",
        "def get_batches(sentences, batch_size, threshold):    \n",
        "    for batch_i in range(0, len(sentences)//batch_size):\n",
        "        start_i = batch_i * batch_size\n",
        "        sentences_batch = sentences[start_i:start_i + batch_size]\n",
        "        \n",
        "        sentences_batch_noisy = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentences_batch_noisy.append(noise_maker(sentence, threshold))\n",
        "            \n",
        "        sentences_batch_eos = []\n",
        "        for sentence in sentences_batch:\n",
        "            sentence.append(vocab_to_int['<EOS>'])\n",
        "            sentences_batch_eos.append(sentence)\n",
        "            \n",
        "        pad_sentences_batch = np.array(pad_sentence_batch(sentences_batch_eos))\n",
        "        pad_sentences_noisy_batch = np.array(pad_sentence_batch(sentences_batch_noisy))\n",
        "        \n",
        "        # Need the lengths for the _lengths parameters\n",
        "        pad_sentences_lengths = []\n",
        "        for sentence in pad_sentences_batch:\n",
        "            pad_sentences_lengths.append(len(sentence))\n",
        "        \n",
        "        pad_sentences_noisy_lengths = []\n",
        "        for sentence in pad_sentences_noisy_batch:\n",
        "            pad_sentences_noisy_lengths.append(len(sentence))\n",
        "        \n",
        "        yield pad_sentences_noisy_batch, pad_sentences_batch, pad_sentences_noisy_lengths, pad_sentences_lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B7ldffWM2uK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The default parameters\n",
        "epochs = 100\n",
        "batch_size = 128\n",
        "num_layers = 2\n",
        "rnn_size = 512\n",
        "embedding_size = 128\n",
        "learning_rate = 0.0005\n",
        "direction = 2\n",
        "threshold = 0.95\n",
        "keep_probability = 0.75"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfRgAPFBM2uP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Method to build the graph\n",
        "def build_graph(keep_prob, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction):\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    \n",
        "    # Load the model inputs    \n",
        "    inputs, targets, keep_prob, inputs_length, targets_length, max_target_length = model_inputs()\n",
        "\n",
        "    # Create the training and inference logits\n",
        "    training_logits, inference_logits = seq2seq_model(tf.reverse(inputs, [-1]),\n",
        "                                                      targets, \n",
        "                                                      keep_prob,   \n",
        "                                                      inputs_length,\n",
        "                                                      targets_length,\n",
        "                                                      max_target_length,\n",
        "                                                      len(vocab_to_int)+1,\n",
        "                                                      rnn_size, \n",
        "                                                      num_layers, \n",
        "                                                      vocab_to_int,\n",
        "                                                      batch_size,\n",
        "                                                      embedding_size,\n",
        "                                                      direction)\n",
        "\n",
        "    # Create tensors for the training logits and inference logits\n",
        "    training_logits = tf.identity(training_logits.rnn_output, 'logits')\n",
        "\n",
        "    with tf.name_scope('predictions'):\n",
        "        predictions = tf.identity(inference_logits.sample_id, name='predictions')\n",
        "        tf.summary.histogram('predictions', predictions)\n",
        "\n",
        "    # Create the weights for sequence_loss\n",
        "    masks = tf.sequence_mask(targets_length, max_target_length, dtype=tf.float32, name='masks')\n",
        "    \n",
        "    with tf.name_scope(\"cost\"):\n",
        "        # Loss function\n",
        "        cost = tf.contrib.seq2seq.sequence_loss(training_logits, \n",
        "                                                targets, \n",
        "                                                masks)\n",
        "        tf.summary.scalar('cost', cost)\n",
        "\n",
        "    with tf.name_scope(\"optimze\"):\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "\n",
        "        # Gradient Clipping\n",
        "        gradients = optimizer.compute_gradients(cost)\n",
        "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
        "        train_op = optimizer.apply_gradients(capped_gradients)\n",
        "\n",
        "    # Merge all of the summaries\n",
        "    merged = tf.summary.merge_all()    \n",
        "\n",
        "    # Export the nodes \n",
        "    export_nodes = ['inputs', 'targets', 'keep_prob', 'cost', 'inputs_length', 'targets_length',\n",
        "                    'predictions', 'merged', 'train_op','optimizer']\n",
        "    Graph = namedtuple('Graph', export_nodes)\n",
        "    local_dict = locals()\n",
        "    graph = Graph(*[local_dict[each] for each in export_nodes])\n",
        "\n",
        "    return graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "X0U8R6SmM2uX",
        "colab_type": "text"
      },
      "source": [
        "### Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "GtyYpr2SM2uY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train the RNN\n",
        "def train(model, epochs, log_string):   \n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        # Used to determine when to stop the training early\n",
        "        testing_loss_summary = []\n",
        "\n",
        "        # Keep track of which batch iteration is being trained\n",
        "        iteration = 0\n",
        "        \n",
        "        display_step = 30 # The progress of the training will be displayed after every 30 batches\n",
        "        stop_early = 0 \n",
        "        stop = 3 # If the batch_loss_testing does not decrease in 3 consecutive checks, stop training\n",
        "        per_epoch = 3 # Test the model 3 times per epoch\n",
        "        testing_check = (len(training_sorted)//batch_size//per_epoch)-1\n",
        "\n",
        "        print()\n",
        "        print(\"Training Model: {}\".format(log_string))\n",
        "\n",
        "        train_writer = tf.summary.FileWriter('./logs/train/{}'.format(log_string), sess.graph)\n",
        "        test_writer = tf.summary.FileWriter('./logs/test/{}'.format(log_string))\n",
        "\n",
        "        for epoch_i in range(1, epochs+1): \n",
        "            batch_loss = 0\n",
        "            batch_time = 0\n",
        "            \n",
        "            for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                    get_batches(training_sorted, batch_size, threshold)):\n",
        "                start_time = time.time()\n",
        "\n",
        "                summary, loss, _ = sess.run([model.merged,\n",
        "                                             model.cost, \n",
        "                                             model.train_op], \n",
        "                                             {model.inputs: input_batch,\n",
        "                                              model.targets: target_batch,\n",
        "                                              model.inputs_length: input_length,\n",
        "                                              model.targets_length: target_length,\n",
        "                                              model.keep_prob: keep_probability})\n",
        "\n",
        "\n",
        "                batch_loss += loss\n",
        "                end_time = time.time()\n",
        "                batch_time += end_time - start_time\n",
        "\n",
        "                # Record the progress of training\n",
        "                train_writer.add_summary(summary, iteration)\n",
        "\n",
        "                iteration += 1\n",
        "\n",
        "                if batch_i % display_step == 0 and batch_i > 0:\n",
        "                    print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(epoch_i,\n",
        "                                  epochs, \n",
        "                                  batch_i, \n",
        "                                  len(training_sorted) // batch_size, \n",
        "                                  batch_loss / display_step, \n",
        "                                  batch_time))\n",
        "                    batch_loss = 0\n",
        "                    batch_time = 0\n",
        "\n",
        "                #### Testing ####\n",
        "                if batch_i % testing_check == 0 and batch_i > 0:\n",
        "                    batch_loss_testing = 0\n",
        "                    batch_time_testing = 0\n",
        "                    for batch_i, (input_batch, target_batch, input_length, target_length) in enumerate(\n",
        "                            get_batches(testing_sorted, batch_size, threshold)):\n",
        "                        start_time_testing = time.time()\n",
        "                        summary, loss = sess.run([model.merged,\n",
        "                                                  model.cost], \n",
        "                                                     {model.inputs: input_batch,\n",
        "                                                      model.targets: target_batch,\n",
        "                                                      model.inputs_length: input_length,\n",
        "                                                      model.targets_length: target_length,\n",
        "                                                      model.keep_prob: 1})\n",
        "\n",
        "                        batch_loss_testing += loss\n",
        "                        end_time_testing = time.time()\n",
        "                        batch_time_testing += end_time_testing - start_time_testing\n",
        "\n",
        "                        # Record the progress of testing\n",
        "                        test_writer.add_summary(summary, iteration)\n",
        "\n",
        "                    n_batches_testing = batch_i + 1\n",
        "                    print('Testing Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
        "                          .format(batch_loss_testing / n_batches_testing, \n",
        "                                  batch_time_testing))\n",
        "                    \n",
        "                    batch_time_testing = 0\n",
        "\n",
        "                    # If the batch_loss_testing is at a new minimum, save the model\n",
        "                    testing_loss_summary.append(batch_loss_testing)\n",
        "                    if batch_loss_testing <= min(testing_loss_summary):\n",
        "                        print('New Record!') \n",
        "                        stop_early = 0\n",
        "                        checkpoint = \"./{}.ckpt\".format(log_string)\n",
        "                        saver = tf.train.Saver()\n",
        "                        saver.save(sess, checkpoint)\n",
        "\n",
        "                    else:\n",
        "                        print(\"No Improvement.\")\n",
        "                        stop_early += 1\n",
        "                        if stop_early == stop:\n",
        "                            break\n",
        "\n",
        "            if stop_early == stop:\n",
        "                print(\"Stopping Training.\")\n",
        "                break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "M4ssEqjdM2ud",
        "colab_type": "code",
        "outputId": "9a1f7b03-2e31-4e2f-b5ab-a8373d4cc63a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Train the model with the desired tuning parameters\n",
        "for keep_probability in [0.75]:\n",
        "    for num_layers in [2]:\n",
        "        for threshold in [0.95]:\n",
        "            log_string = 'kp={},nl={},th={}'.format(keep_probability,\n",
        "                                                    num_layers,\n",
        "                                                    threshold) \n",
        "            model = build_graph(keep_probability, rnn_size, num_layers, batch_size, \n",
        "                                learning_rate, embedding_size, direction)\n",
        "            train(model, epochs, log_string)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training Model: kp=0.75,nl=2,th=0.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Yjqx0GbmM2uh",
        "colab_type": "text"
      },
      "source": [
        "## Fixing Custom Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynwxtRGfM2ui",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare the text for the model\n",
        "def text_to_ints(text):  \n",
        "    \n",
        "    text = clean_text(text)\n",
        "    return [vocab_to_int[word] for word in text]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpjSibMtM2uq",
        "colab_type": "text"
      },
      "source": [
        "### example01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgeeaGpYM2ur",
        "colab_type": "code",
        "outputId": "4bce8c99-1439-447d-a534-49ec073bee9c",
        "colab": {}
      },
      "source": [
        "# Create your own sentence or use one from the dataset\n",
        "text = \"The first days of her existence in th country were vrey hard for Dolly..\"\n",
        "text = text_to_ints(text)\n",
        "\n",
        "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Load saved model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqCQQfNYM2uv",
        "colab_type": "code",
        "outputId": "a01ff4a9-1dfc-4afd-9ab9-840bfa3615c1",
        "colab": {}
      },
      "source": [
        "# Remove the padding from the generated sentence\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "# Text\n",
        "print('\\nText')\n",
        "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Text\n",
            "  Input Words: The first days of her existence in th country were vrey hard for Dolly. . \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4O2kNZcRM2u6",
        "colab_type": "code",
        "outputId": "099baf49-134f-4ee7-814e-9561caa32ef0",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "print('\\nSummary')\n",
        "print('Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Summary\n",
            "Response Words: The first days of her existence in th country were vere vre hard for Dolly.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b91sNYBGM2u_",
        "colab_type": "text"
      },
      "source": [
        "### example02"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecWNpR-RM2vA",
        "colab_type": "code",
        "outputId": "e42548a3-1af7-45f7-d1d5-e84471e6d5e8",
        "colab": {}
      },
      "source": [
        "# Create your own sentence or use one from the dataset\n",
        "text = \"Thi is really something impressiv thaat we should look into right away!\"\n",
        "text = text_to_ints(text)\n",
        "\n",
        "checkpoint = \"./kp=0.75,nl=2,th=0.95.ckpt\"\n",
        "\n",
        "model = build_graph(keep_probability, rnn_size, num_layers, batch_size, learning_rate, embedding_size, direction) \n",
        "\n",
        "with tf.Session() as sess:\n",
        "    # Load saved model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, checkpoint)\n",
        "    \n",
        "    #Multiply by batch_size to match the model's input parameters\n",
        "    answer_logits = sess.run(model.predictions, {model.inputs: [text]*batch_size, \n",
        "                                                 model.inputs_length: [len(text)]*batch_size,\n",
        "                                                 model.targets_length: [len(text)+1], \n",
        "                                                 model.keep_prob: [1.0]})[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./kp=0.75,nl=2,th=0.95.ckpt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipuWW4YoM2vI",
        "colab_type": "code",
        "outputId": "c1698ac1-bbdb-4658-c67a-20ad6bc0f9ad",
        "colab": {}
      },
      "source": [
        "# Remove the padding from the generated sentence\n",
        "pad = vocab_to_int[\"<PAD>\"] \n",
        "# Text\n",
        "print('\\nText')\n",
        "print('  Input Words: {}'.format(\"\".join([int_to_vocab[i] for i in text])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Text\n",
            "  Input Words: Thi is really something impressiv thaat we should look into right away! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwhI9hvVM2vP",
        "colab_type": "code",
        "outputId": "a323f346-19c4-4fb4-b569-11883715abcf",
        "colab": {}
      },
      "source": [
        "# Summary\n",
        "print('\\nSummary')\n",
        "print('Response Words: {}'.format(\"\".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Summary\n",
            "Response Words: Thi is really something impressive that we should look into right away! i\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "yb4DCY9sM2vU",
        "colab_type": "text"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV-5VGLzM2vX",
        "colab_type": "text"
      },
      "source": [
        "I hope that you have found this project to be rather interesting and useful. The example sentences that I have presented above were specifically chosen, and the model will not always be able to make corrections of this quality. Given the amount of data that we are working with, this model still struggles. For it to be more useful, it would require far more training data, and additional parameter tuning. This parameter values that I have above worked best for me, but I expect there are even better values that I was not able to find."
      ]
    }
  ]
}
